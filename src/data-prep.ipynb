{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-12T15:47:16.679722Z",
     "start_time": "2025-05-12T15:47:14.694293Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/domains-and-seqs-merged.csv\")"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T15:47:17.089122Z",
     "start_time": "2025-05-12T15:47:16.680501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "MIN_DOMAINS_PER_HOMOLOGY = 10\n",
    "MAX_DOMAINS_PER_HOMOLOGY = 200\n",
    "\n",
    "HOMOLOGY_GROUPS = 100\n",
    "SAMPLES_PER_GROUP = 10\n",
    "\n",
    "# Define hierarchy columns â€” this full path defines a unique homology group\n",
    "hierarchy = ['class', 'architecture', 'topology', 'homology']\n",
    "\n",
    "# Step 1 & 2: Filter groups where the number of domain_id entries is at least 100\n",
    "filtered_df = df.groupby(hierarchy).filter(lambda x: MIN_DOMAINS_PER_HOMOLOGY <= len(x) <= MAX_DOMAINS_PER_HOMOLOGY)\n",
    "\n",
    "# Step 3: Get unique full-path homology groups\n",
    "unique_homology_paths = filtered_df[hierarchy].drop_duplicates()\n",
    "\n",
    "# Randomly sample 100 unique homology groups (based on full path)\n",
    "sampled_paths = unique_homology_paths.sample(n=HOMOLOGY_GROUPS, random_state=42)\n",
    "\n",
    "# Step 4: Retain only rows that belong to the sampled groups\n",
    "sampled_df = pd.merge(sampled_paths, filtered_df, on=hierarchy)\n",
    "\n",
    "# Within each sampled group, randomly choose 100 domain_id entries\n",
    "subset = sampled_df.groupby(hierarchy).apply(lambda x: x.sample(n=SAMPLES_PER_GROUP, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# final_df is your result\n",
    "subset.to_csv(\"../data/subset.csv\", index=False)\n"
   ],
   "id": "eebda538a6f803d9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6p/_m5tm17x05qfp8tftlrxjzd00000gn/T/ipykernel_59732/612984342.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subset = sampled_df.groupby(hierarchy).apply(lambda x: x.sample(n=SAMPLES_PER_GROUP, random_state=42)).reset_index(drop=True)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T15:47:42.454452Z",
     "start_time": "2025-05-12T15:47:19.324074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "\n",
    "# Load ProtT5 model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\", do_lower_case=False)\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "model = model.eval()\n",
    "\n",
    "# Check if CUDA is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ],
   "id": "790dd24eb634af3a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T15:52:10.102287Z",
     "start_time": "2025-05-12T15:52:05.271571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dictionary to store embeddings\n",
    "all_embeddings = {}\n",
    "\n",
    "# Process each sequence\n",
    "for index, row in subset.head(20).iterrows():\n",
    "    sequence = row[\"sequence\"]\n",
    "    sequence = sequence.replace('U', 'X').replace('Z', 'X').replace('O', 'X')\n",
    "    ids = tokenizer.batch_encode_plus([sequence], add_special_tokens=True, padding=True, return_tensors=\"pt\")\n",
    "    input_ids = ids['input_ids'].to(device)\n",
    "    attention_mask = ids['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Average over tokens to get a single vector per sequence\n",
    "    sequence_embedding = embedding.last_hidden_state.mean(dim=1).squeeze().cpu()\n",
    "\n",
    "    # Store in dictionary\n",
    "    all_embeddings[index] = sequence_embedding\n",
    "\n",
    "    print(f\"Processed: {index}\")\n",
    "\n",
    "# Save all embeddings to one file\n",
    "torch.save(all_embeddings, \"../data/all_embeddings.pt\")\n",
    "print(\"All embeddings saved to all_embeddings.pt\")\n"
   ],
   "id": "f38bc03943fdc6cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 0\n",
      "Processed: 1\n",
      "Processed: 2\n",
      "Processed: 3\n",
      "Processed: 4\n",
      "Processed: 5\n",
      "Processed: 6\n",
      "Processed: 7\n",
      "Processed: 8\n",
      "Processed: 9\n",
      "Processed: 10\n",
      "Processed: 11\n",
      "Processed: 12\n",
      "Processed: 13\n",
      "Processed: 14\n",
      "Processed: 15\n",
      "Processed: 16\n",
      "Processed: 17\n",
      "Processed: 18\n",
      "Processed: 19\n",
      "All embeddings saved to all_embeddings.pt\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4774b13e968a8152"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
